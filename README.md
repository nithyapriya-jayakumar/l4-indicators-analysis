# L4 Indicators Analysis  
This repository contains my implementation and evaluation of four L4 ethical AI indicators as part of the AI Ethics Assignment.  
Each indicator is implemented in a separate folder with its own dataset, model execution script, and evaluation pipeline.

---

## üìÇ Project Structure

l4-indicators-analysis/
‚îÇ
‚îú‚îÄ‚îÄ factuality_hallucination/
‚îú‚îÄ‚îÄ uncertainty_confidence/
‚îú‚îÄ‚îÄ knowledge_attribution_l4/
‚îî‚îÄ‚îÄ citation_evidencelinks/


Each folder includes:
- `run_model.py` ‚Äì main script for running model inference  
- `evaluation.py` ‚Äì scoring and evaluation script  
- dataset files (JSON/CSV/JSONL)  
- response/output files  

---

#  L4 Indicators Overview

## 1Ô∏è Factuality / Hallucination  
Measures how accurately the model generates information and identifies hallucinated content.  
**Folder:** `factuality_hallucination/`

---

## 2Ô∏è Uncertainty & Confidence  
Evaluates whether the model expresses confidence appropriately relative to its output.  
**Folder:** `uncertainty_confidence/`

---

## 3Ô∏è Knowledge Attribution (L4)  
Assesses whether the model properly attributes its answers to the correct sources.  
(Example dataset: HotpotQA ‚Äî not included due to size limits.)  
**Folder:** `knowledge_attribution_l4/`

---

## 4Ô∏è Citation & Evidence Links  
Checks whether the model provides evidence-backed citations and links statements to supporting information.  
**Folder:** `citation_evidencelinks/`

---

##  Datasets

Large benchmark datasets such as `hotpot_dev_distractor_v1.json` are **not uploaded** due to GitHub file size limits.  
Download from the official source if needed:

üîó HotpotQA Dataset: https://github.com/hotpotqa/hotpot

Place datasets into their corresponding indicator folders before running code.

---

##  API Key Setup (.env)

Create a `.env` file in the project root and add the API keys used in this project:

GROQ_API_KEY=your_key_here
OPENROUTER_API_KEY=your_key_here
DEEPSEEK_API_KEY=your_key_here


The `.env` file is ignored by GitHub for security and must not be uploaded.  
Scripts automatically load these values using `python-dotenv`.

---
## üß™ Evaluation Metrics (BLEURT & COMET)

Some indicators use advanced text evaluation metrics:

- **BLEURT** ‚Äì A learned evaluation metric for assessing factuality and semantic similarity.  
- **COMET** ‚Äì A neural evaluation metric commonly used for translation and reliability scoring.

These are installed automatically through `requirements.txt`.  
Evaluation scripts run BLEURT or COMET only for indicators that require them.

---

##  Running the Code

### Installation
pip install -r requirements.txt


### Run model inference


python run_model.py


### Evaluate results


python evaluation.py


Each indicator folder runs independently and produces its own output files.

---

##  Notes
- `.env` is excluded using `.gitignore` to protect API keys.  
- Large datasets are not included in the repository.  
- Each indicator is structured independently for clarity.

---

## License
This repository is for academic and research use only.
 
